#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Knuteon!
# Copyright (C) 2019 by Sven Kochmann

# Program name was generated by chem-name-gen, see 
# DOI: 10.5281/zenodo.2578429

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the  Free Software Foundation,  either version 3  of the License, or
# (at your option) any later version.

# This program  is distributed  in the hope  that it will  be  useful,
# but  WITHOUT  ANY  WARRANTY;  without even  the implied warranty  of
# MERCHANTABILITY  or  FITNESS  FOR  A  PARTICULAR  PURPOSE.  See  the
# GNU General Public License for more details.

# You  should  have  received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

# This program extracts the data lines from Karat32 data files.  These
# data files are simple OLE files with the following structure:
# 
# experiment.dat
# 	|-> AuditTrail
# 	|-> Baseline Check
# 	|-> ChangeLog
# 	|-> Detector Data          <-- the folder with the detector traces
# 	|-> MethodXX
# 	|-> Results
# 	|-> Signature
# 	|-> XY Data
# 	[5]SummaryInformation
# 	3D Data
# 	3D Trace Handler
# 	Chrom Header		       <-- header file containing general info
#	Contents				   <-- list of traces
#	Detector Trace Handler
#	Extra Sequence Columns
#	FileValidation
#	Multichromatogram Info
#	ResultEntries
#	XY Trace Handler

# Imports
import argparse
import datetime
import math
import olefile
import os
import pathlib
import pwexplode
import signal
import struct

# This fixes Python's default behaviour  of throwing an exception when
# pipe reading (e.g. with 'head')  suddenly stops. Who though this was
# a good default behaviour anyway?  This will  not work under Windows.
if os.name is 'posix':
	signal.signal(signal.SIGPIPE, signal.SIG_DFL)


# Main program of Knuteon!
def main():
	# Argument setup and parsing
	parser = argparse.ArgumentParser(
				description = 'Knuteon! This program extracts the data from Karat32 data files.')

	parser.add_argument('inputfile', action = 'store', nargs = 1, type = str)

	parser.add_argument('-v', '--version', help = 'prints version information', action = 'version', version = 'Knuteon! 1.0 by Sven Kochmann')
	
	group = parser.add_mutually_exclusive_group()
	group.add_argument('-a', '--all', help = 'extracts all detector traces and saves them as files', action = 'store_true', default = 'True')
	group.add_argument('-f', '--list_files', help = 'lists all files and streams in the data file', action = 'store_true')
	group.add_argument('-i', '--print_header', help = 'prints only the header information (chrom header)', action = 'store_true')
	group.add_argument('-t', '--list_traces', help = 'lists all traces in the data file', action = 'store_true')

	group.add_argument('-e', '--extract_trace', metavar = 'ID', help = 'extracts the single trace given by ID and prints it to stdout', type = str, default = '')

	args = vars(parser.parse_args())

	# Opening of file, reading of header information
	if olefile.isOleFile(args['inputfile'][0]) == False:
		print("%s is not an ole compound file, so it cannot be a Karat32 data file. Sorry." % args['inputfile'][0])
		exit()

	ole = olefile.OleFileIO(args['inputfile'][0])
	basename = os.path.splitext(args['inputfile'][0])[0]

	if args['list_files']:
		tree = []
		files = ole.listdir(True)

		for item in files:
			tree = add_sublist_to_tree(tree, item)
	
		print(args['inputfile'][0])
		list_data_files(tree, 0)

		print("")
		print("%d files and streams found." % (len(files)))		
		print("")
		exit()

	header = read_chrom_header(ole)
	header.update({'filename': basename})

	if args['print_header']:	
		for key in sorted(header):
			print("%12s: %s" % (key.capitalize(), header[key]))
		print("")
		exit()

	traces = read_traces(ole)

	if args['list_traces']:
		for trace in traces:
			print('Detector Data/Detector %s Trace' % (trace['id']))
			for key in sorted(trace):
				if '(internal)' not in key:
					print("%15s: %s" % (key.capitalize(), trace[key]), end = '')
					if key == 'maximum time':
						print(" s")
					elif key == 'sample rate':
						print(" Hz")
					else:
						print("")
			print("")

		print("%d traces found." % (len(traces)))		
		print("")
		exit()

	if len(args['extract_trace']) > 0:
		traceheader = {'id': ''}
		for trace in traces:
			if trace['id'] == args['extract_trace']:
				traceheader = trace
				break

		if len(traceheader['id']) == 0:
			print("Error: No trace with ID %s found." % (args['extract_trace']))
			exit()

		data = extract_trace(ole, traceheader)
		print("%8s\t%8s (%s)" % ("Time (s)", traceheader['y-axis name'], traceheader['y-axis unit']))
		for point in data:
			print("%8.2f\t%8.5e" % (point[0], point[1]))
		print("%d data points acquired." % (len(data)))
		print("")
		exit()

	# Main and default function:  write all traces to individual files
	# including description and header	
	if args['all']:
		for trace in traces:
			outputname = basename + "." + trace['id'] + ".txt"
			print("Writing %s..." % (outputname))

			with open(outputname, 'w') as tracefile:
				for key in sorted(header):
					tracefile.write("%12s: %s\n" % (key.capitalize(), header[key]))
				tracefile.write("\n")
				
				tracefile.write("Detector Data/Detector %s Trace\n" % (trace['id']))
				for key in sorted(trace):
					if '(internal)' not in key:
						tracefile.write("%15s: %s" % (key.capitalize(), trace[key]))
						if key == 'maximum time':
							tracefile.write(" s")
						elif key == 'sample rate':
							tracefile.write(" Hz")
						tracefile.write("\n")
				tracefile.write("\n")

				data = extract_trace(ole, trace)
				tracefile.write("%s\t%s (%s)\n" % ("Time (s)", trace['y-axis name'], trace['y-axis unit']))
				for point in data:
					tracefile.write("%f\t%e\n" % (point[0], point[1]))

				tracefile.write("\n\n")

		print("%d traces written." % (len(traces)))	
		print("")
		exit()


######################################################################
# Extracts  the actual trace data  and  returns it as  list of tuples.
# This function needs  the header information gotten by "read_traces".
# Data  might  actually be compressed by PKWARE compression,  which is
# handled by the pwexplode library. 
def extract_trace(ole, header):
	datapoints = []
	datastream = ole.openstream(['Detector Data', 'Detector %s Trace' % header['id']])

	# The stream starts  with 5 integers:  version,  number of points,
	# maximum points, channels, compression flag. Right now, we ignore
	# everything except the number of points and the compression flag.
	version = struct.unpack('<I', datastream.read(4))[0]
	nopoints = struct.unpack('<I', datastream.read(4))[0]
	maxpoints = struct.unpack('<I', datastream.read(4))[0] 
	channels = struct.unpack('<I', datastream.read(4))[0]
	compression = struct.unpack('<I', datastream.read(4))[0]

	decompresseddata = ""
	# Compression is 1  if PWWARE compression  was used  for the data.
	# Other compression methods are not known or not used.
	if compression == 1:
		# The data is chunked into blocks of 2048. As soon as we read
		# a block of less, we have all the data! 
		readablock = True
		compresseddata = ""
		while readablock:
			length = struct.unpack('<H', datastream.read(2))[0]
			compresseddata += datastream.read(length)

			if not length == 2048:
				readablock = False

		# Decompress all the data
		decompresseddata = pwexplode.explode(compresseddata)
	else:
		# Each data point is an integer of 4 bytes, read all in here
		decompresseddata = datastream.read(nopoints * 4)

	# Convert  the list of integers  into (time, signal)-tuples.  Time 
	# is given by  the index of  the data point  and  the sample rate,
	# while  the actual signal is given by  the data point integer and
	# the multiplier. Note, that the time is actually saved in seconds
	# but always presented as minutes. We keep seconds here.
	pos = 0
	for i in range(nopoints):
		datapoint = struct.unpack('<i', decompresseddata[pos:pos + 4])[0]
		pos += 4

		time = i / header['sample rate']
		signal = datapoint * header['multiplier']
		
		datapoints.append((time, signal))

	return datapoints


######################################################################
# Reads all traces  in the given data file  and returns them as a list
# of dictionaries
def read_traces(ole):
	# A blank trace dictionary
	trace_dict = {'id': '', 'y-axis name': '', 'y-axis name (internal)': '', 'y-axis unit': '', 'y-axis unit (internal)': '',
				  'sample rate': 0.0, 'multiplier': 1.0, 'x-axis name': '', 'x-axis name (internal)': '', 'maximum time': 0.0}
	traces = []

	# Traces are all saved in 'Detector Trace Handler',  so let's read
	# it and collect the information in there
	data_dth = ole.openstream('Detector Trace Handler')

	# Unfortunately, the first 20 Bytes are unknown, just skip
	data_dth.read(20)

	# 2 Bytes (Word) give  the number of signals followed by 4 unknown
	# bytes (that are usually FFFF0000), which we skip
	nosignals = struct.unpack('<H', data_dth.read(2))[0]
	data_dth.read(4)

	# Next up  is  an identifier string (length + string itself)  that
	# should always be 'CDetTracInfo'.  This is a soft check for this,
	# i.e. if it is NOT we just print a warning but go on!
	length = struct.unpack('<H', data_dth.read(2))[0]
	identifier = data_dth.read(length).decode()
	if identifier != "CDetTraceInfo":
		print("Warning: 'Detector Trace Handler' is not of type 'CDetTraceInfo'")
	
	# Now, there is a block for each signal
	for i in range(nosignals):
		# Create a COPY of the empty dictionary!
		tracedata = trace_dict.copy()

		# 4 Bytes are unknown, just skip
		data_dth.read(4)

		# These 4 bytes give some sort of  type number, followed again
		# by 4 unknown bytes, which we skip
		typeno = struct.unpack('<I', data_dth.read(4))[0]
		data_dth.read(4)

		# There are two known types so far: 8 and 9, which just define
		# how the detector trace ID is encoded:
		# Type 8: trace ID = integer
		# Type 9: trace ID = string
		if typeno == 8:
			tracedata['id'] = str(struct.unpack('<I', data_dth.read(4))[0])
		elif typeno == 9:
			length = struct.unpack('B', data_dth.read(1))[0]
			tracedata['id'] = data_dth.read(length).decode()
		else:
			print("Warning: type %d unknown for signal trace %d" % (typeno, i+1))

		# 4 Bytes are unknown, just skip
		data_dth.read(4)

		# Next bytes encode signal name (y-axis), sample rate (in Hz), 
		# unit (y-axis), multiplier, x-axis name, maximum time, y-axis
		# name  (internal),   y-axis  unit  (internal),   x-axis  name 
		# (internal)
		length = struct.unpack('B', data_dth.read(1))[0]
		tracedata['y-axis name'] = data_dth.read(length).decode()

		tracedata['sample rate'] = (1.0/float(struct.unpack('<f', data_dth.read(4))[0]))

		length = struct.unpack('B', data_dth.read(1))[0]
		tracedata['y-axis unit'] = data_dth.read(length).decode('cp1252', errors = 'ignore')

		tracedata['multiplier'] = float(struct.unpack('<f', data_dth.read(4))[0])

		# Technically,  this will usually lead to  'Minutes'.  However 
		# the actual data is saved and decoded in seconds.  Therefore,
		# we replace it here by the more accurate 'Time (s)'.
		length = struct.unpack('B', data_dth.read(1))[0]
		data_dth.read(length)
		tracedata['x-axis name'] = "Time (s)"
		
		# These  are  two floats (4 bytes each).  The first one is not
		# known, so we just skip it for now.  It could be the steps in
		# minutes  for  each  data point  acquired  (redundant  sample 
		# rate).  The second one is  the maximum time (in seconds) set  
		# by  the user  when setting up  the method.  But it  does not 
		# mean this time was reached!  Indeed,  the user can stop  the 
		# measurement at any time giving less  data points in the end.
		# These floats are followed by 24 unknown bytes, which we just 
		# ignore.
		struct.unpack('<f', data_dth.read(4))[0]
		tracedata['maximum time'] = struct.unpack('<f', data_dth.read(4))[0]
		data_dth.read(24)

		length = struct.unpack('B', data_dth.read(1))[0]
		tracedata['y-axis name (internal)'] = data_dth.read(length).decode()

		length = struct.unpack('B', data_dth.read(1))[0]
		tracedata['y-axis unit (internal)'] = data_dth.read(length).decode('cp1252', errors = 'ignore')

		# 8 Bytes are unknown, just skip
		data_dth.read(8)

		length = struct.unpack('B', data_dth.read(1))[0]
		tracedata['x-axis name (internal)'] = data_dth.read(length).decode()

		# Last 26 bytes are unknown, skip and go one with next trace
		data_dth.read(26)

		# Add to output
		traces.append(tracedata)		

	return traces


######################################################################
# Converts recursively  a list in  the form ['X', 'Y', 'Z', 'file'] to
# a   embeded   list,   which   can  then   be  recursively  shown  by 
# list_data_files (see below)
def add_sublist_to_tree(tree, item):
	if type(item) is not list:
		return tree

	if len(item) == 0:
		return tree

	index = -1
	for i, _ in enumerate(tree):
		if item[0] == tree[i][0]:
			index = i
			break

	if index == -1:
		tree.append([item[0], []])

	if len(item) > 1:
		tree[index][1] = add_sublist_to_tree(tree[index][1], item[1:])

	return tree


######################################################################
# Prints  a directory  structure  of  the  given list;  can be  called 
# recursively with a depth parameter for sublists
def list_data_files(items, depth):
	for item in items:
		if type(item) is not list:
			continue

		print(" " * (1 + depth * 3) + "↳", end = ' ')

		print(item[0])

		if len(item[1]) > 0:				
			list_data_files(item[1], depth + 1)	
		

######################################################################
# Converts an OLE variant time double to system time and returns it as 
# a datetime object. Math based on the description at
# https://docs.microsoft.com/en-us/dotnet/api/system.datetime.tooadate
def variant_to_system_time(oletime):
	# Date is before the period, the time afterwards; we have to check 
	# for negative times/dates; we add 1e-11 to force double precision
	# for the time part
	datepart = (math.ceil(oletime) if oletime < 0.0 else math.floor(oletime))
	timepart = math.fabs(oletime - datepart + 1e-11)

	if timepart >= 1.0:
		timepart -= 1e-11

	# Convert date part to days, months, and years; the reference date
	# for OLE variant times is December 30, 1899.
	realdate = datetime.datetime(1899, 12, 30) + datetime.timedelta(days=int(datepart))	

	# Time  is  represented  as  fraction  of 24 hours  with .00 being 
	# 0:00 am, .25 being 6:00 am, etc. We just multiply	the value with 
	# 24 hours á 60 minutes á 60 seconds  and add it to the date.
	realdate = realdate + datetime.timedelta(seconds=int(timepart * 24 * 3600))

	return realdate


######################################################################
# Reads 'chrom header' from the OLE data file and returns its contents
# as a dictionary
def read_chrom_header(ole):
	headerinfo = {'runtime': datetime.datetime(1899, 12, 30), 'method name': '', 'method path': '', 'description': '', 
				  'version': '', 'system': '', 'detector': ''}

	chromhead = ole.openstream('Chrom Header')

	# First 8 bytes of this file are unknown, just read to skip
	chromhead.read(8)

	# Next 8 bytes encode the runtime as a double (OLE variant date)
	headerinfo['runtime'] = struct.unpack('<d', chromhead.read(8))[0]
	headerinfo['runtime'] = variant_to_system_time(headerinfo['runtime'])

	# Next  bytes  contain  the strings  for  the method  file,  data
	# description,  version info,  and system info  each encoded as a 
	# C-string (length as a byte + the string itself, max 255 bytes)
	length = struct.unpack('B', chromhead.read(1))[0]
	method = pathlib.PureWindowsPath(chromhead.read(length).decode())
	headerinfo['method name'] = method.name
	headerinfo['method path'] =	method.parent

	length = struct.unpack('B', chromhead.read(1))[0]
	headerinfo['description'] = chromhead.read(length).decode()

	length = struct.unpack('B', chromhead.read(1))[0]
	headerinfo['version'] = chromhead.read(length).decode()

	length = struct.unpack('B', chromhead.read(1))[0]
	headerinfo['system'] = chromhead.read(length).decode()

	# Next 22 bytes are unknown, just read to skip
	chromhead.read(22)

	# Detector string encoded as C-String (see above)
	length = struct.unpack('B', chromhead.read(1))[0]
	headerinfo['detector'] = chromhead.read(length).decode()

	return headerinfo


######################################################################
# Little trick  to allow  the main program  in the beginning  but  all
# the function definitions at the end
if __name__ == '__main__':
    main()




